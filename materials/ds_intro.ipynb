{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup\n",
    "- Check if python3 installed: `python3 --version`\n",
    "    - If not: https://www.python.org/downloads/\n",
    "- Create a directory for this class. From that directory, create and activate a virtual environment:\n",
    "    - `python3 -m venv responsible-ml-env`\n",
    "    - `source responsible-ml-env/bin/active`\n",
    "- Install data science / ML packages: \n",
    "    - `pip3 install jupyterlab pandas scikit-learn torch transformers`\n",
    "- Download this notebook from the course website and place it in the class directory. Launch it with:\n",
    "    - `python3 -m jupyter lab`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**: Google/Jigsaw's conversation AI team built something called *Perspective API*, which is a publicly accessible toxicity detection model for text. In 2018, they sponsored a Kaggle competition to incentivize the creation of better models to predict specific types of toxicity (e.g. obscenity vs. identity-based hate). The data consists of Wikipedia comments labeled by human raters for toxic behavior. \n",
    "\n",
    "*Content Warning: the dataset for this competition contains text that may be considered profane, vulgar, or offensive.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from [Kaggle](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data). Unzip into same directory as notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"jigsaw-toxic-comment-classification-challenge/train.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at some random example data points\n",
    "data.sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: What is our input variable?\n",
    "\n",
    "**Question 2**: What are our outcome variables? \n",
    "\n",
    "**Question 3** What kind of outcome variables do we have (continuous, ordinal, binary)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many positive examples of each outcome variable do we have?\n",
    "outcomes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "data[outcomes].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the distribution of each outcome variable?\n",
    "data[outcomes].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: What are some ethically relevant things we don't know about this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We're taking a small sample of the data so neural network prediction and training doesn't take too long\n",
    "# We're focusing on one outcome variable: toxic\n",
    "sample = data.sample(2000, random_state=0) \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    sample[\"comment_text\"], \n",
    "    sample[\"toxic\"], \n",
    "    test_size=0.5,\n",
    "    random_state=0,\n",
    "    stratify=sample[\"toxic\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**: Why is it important to split our dataset into a separate training and validation subsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 1**: What is cross validation and how is it different from what we did above? What is one benefit it has over a single split of the data? What is one drawback?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. First, we have to decide how to represent our complex, high-dimensional input feature (the comment text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One simple representation is called a \"bag of words\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words are in our vocabulary?\n",
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our one comment in the sample\n",
    "X_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And its \"bag of words\" representation (very sparse)\n",
    "X_train_bow[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Second, we have to decide how to model `Pr(toxic|comment_text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a baseline model for binary outcomes is logistic regression (https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(penalty=None, max_iter=1000)\n",
    "logreg = logreg.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we'll use accuracy to evaluate our model\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_val_bow = vectorizer.transform(X_val)\n",
    "y_pred = logreg.predict(X_val_bow)\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**: 90% accuracy! Is this good performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we predicted 0 for every example?\n",
    "accuracy_score(y_val, [0] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at two other better metrics: precision and recall\n",
    "# https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(f\"precision: {precision_score(y_val, y_pred)}\")\n",
    "print(f\"recall: {recall_score(y_val, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**: Should we care more about precision or recall for detecting toxic comments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 2**: Another useful metric to evaluate a binary classifier is the ROC curve. Plot the ROC curve and compute the closely related ROC-AUC score. (*Hint: this [section](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc) is helpful*). How is the ROC-AUC score related to the ROC curve? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's replace our BOW representation with vectors from a pretrained language model\n",
    "# https://jalammar.github.io/illustrated-bert/\n",
    "# https://huggingface.co/nreimers/MiniLM-L6-H384-uncased\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "model = AutoModel.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "model = model.eval() # some layers behave differently in training vs. prediction/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to convert text into input tokens\n",
    "def tokenize(comment):\n",
    "    return tokenizer(\n",
    "        comment, \n",
    "        truncation=True, \n",
    "        max_length=100, \n",
    "        padding=\"max_length\", \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "X_train_tokenized = [tokenize(comment) for comment in X_train]\n",
    "X_val_tokenized = [tokenize(comment) for comment in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tokens are in our vocabulary?\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at one tokenized example\n",
    "X_train_tokenized[1][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized[1][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# now we'll use the pretrained model to convert the tokens into a dense vector representation\n",
    "def vectorize(comments):\n",
    "    vecs = []\n",
    "    for comment in comments:\n",
    "        with torch.no_grad():\n",
    "            vec = model(**comment)[\"pooler_output\"]\n",
    "        vec = vec.detach().numpy()\n",
    "        vecs.append(vec)\n",
    "    return np.concatenate(vecs)\n",
    "\n",
    "X_train_vecs = vectorize(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we'll fit the same type of model to this other representation of the inputs\n",
    "logreg = logreg.fit(X_train_vecs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's vectorize the validation comments and see how good our predictions are\n",
    "X_val_vecs = vectorize(X_val_tokenized)\n",
    "y_pred = logreg.predict(X_val_vecs)\n",
    "print(f\"precision: {precision_score(y_val, y_pred)}\")\n",
    "print(f\"recall: {recall_score(y_val, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**: Features from the pretained language model performed worse!? Why might this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 3**: Use the original BERT model instead of the smaller model we used in class (use the string \"bert-base-cased\"). Note: vectorizing the text will take longer because BERT is a bigger model (it took ~4 minutes on my computer). How did precision, recall, and ROC-AUC change when using the representations from this model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 4**: Look at BERT's [model card](https://huggingface.co/bert-base-uncased). What data was BERT trained on? What were its two training objectives? How does this influence what biases it might have learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling Approach 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll finetune (i.e. update) the model specifically for our classification task\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch (developed by Facebook) is one framework for training neural networks\n",
    "# it requires you to define a custom dataset class for your data \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, vecs, labels):\n",
    "        self.vecs = vecs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val.reshape(-1) for key, val in self.vecs[idx].items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CommentDataset(X_train_tokenized, y_train)\n",
    "val_dataset = CommentDataset(X_val_tokenized, y_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5) # we'll use a popular variant of stochastic gradient descent to optimize the parameters \n",
    "model.train()\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    outputs = model(**batch) # predict with current model parameters\n",
    "    loss = outputs.loss # for classification: cross entropy loss\n",
    "    loss.backward() # backpropagation (ie which nodes in the network were responsible for the loss)\n",
    "\n",
    "    optimizer.step() # update model parameters: one step in the direction of the gradient of the loss\n",
    "    optimizer.zero_grad() # clears, or zeros, the current gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we'll make predictions on our validation dataset\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "for batch in val_dataloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    preds = torch.argmax(outputs.logits, dim=-1)\n",
    "    all_preds.append(preds.detach().numpy())\n",
    "    all_labels.append(batch[\"labels\"].detach().numpy())\n",
    "    \n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and see how we did!\n",
    "print(f\"precision: {precision_score(all_labels, all_preds)}\")\n",
    "print(f\"recall: {recall_score(all_labels, all_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 5**: If you restart the notebook and re-run modeling approach 3 (fine-tuning) do you get the same precision and recall scores on the validation dataset? Why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
